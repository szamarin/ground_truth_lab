{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This sample notebook takes you through an end-to-end workflow to demonstrate the functionality of SageMaker Ground Truth. We'll leverage SageMaker core functionality along with Ground Truth to train and deploy a basic facial recognition model. In our Ground Truth labelling job, we'll present our labelers with a pair of photos and ask them to identify if the faces in photos are of the same or different person. We'll then train a siamese network model that given two face photos as inputs, will be able to tell us if these are of the same person.\n",
    "\n",
    "### Datasets Used\n",
    "Rather than splitting a single set of facial images, we'll use two completely distinct facial image datasets for training and evaluating the model\n",
    "- Model Training: [AT&T Database of Faces](https://www.kaggle.com/kasikrit/att-database-of-faces)\n",
    "- Model Evaluation: [Yale Face Database](https://www.kaggle.com/olgabelitskaya/yale-face-database)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the default role for data access and job execution\n",
    "role = get_execution_role()\n",
    "\n",
    "# Manage interactions with the Amazon SageMaker APIs and any other AWS services needed.\n",
    "sess = sagemaker.Session()\n",
    "sm_client = sess.boto_session.client('sagemaker')\n",
    "\n",
    "# uses a default bucket created by sagemaker\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Region of our account\n",
    "region = sess.boto_region_name\n",
    "\n",
    "# name of our labeling job\n",
    "labeling_job_name = \"face-labeling\"\n",
    "\n",
    "# path to where we'll copy all of the data\n",
    "s3_root_path = os.path.join(\"s3://\", bucket, \"ground_truth_lab\")\n",
    "\n",
    "# path to data that we'll use in our Ground Truth Labeling Job\n",
    "job_data_path = os.path.join(s3_root_path, \"face_labeling_job_images\")\n",
    "\n",
    "# path to configuration files we'd need to setup our labeling job via SDK\n",
    "labeling_job_config_path = os.path.join(s3_root_path, \"gt_config\")\n",
    "\n",
    "# path to data that's already been labeled\n",
    "labeled_data_path = os.path.join(s3_root_path, \"labeled\")\n",
    "\n",
    "# Ground Truth lambda ARNs - needed to setup job through SDK\n",
    "pre_annotation_lambdas = {\"us-east-1\": \"arn:aws:lambda:us-east-1:432418664414:function:PRE-ImageMultiClass\", \n",
    "                  \"us-east-2\": \"arn:aws:lambda:us-east-2:266458841044:function:PRE-ImageMultiClass\"}\n",
    "\n",
    "consolidation_lambdas = {\"us-east-1\":\"arn:aws:lambda:us-east-1:432418664414:function:ACS-ImageMultiClass\", \n",
    "                 \"us-east-2\": \"arn:aws:lambda:us-east-2:266458841044:function:ACS-ImageMultiClass\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a manifest files which contains the s3 paths to the images we wish to annotate\n",
    "with open(\"gt_config/input.manifest\", \"w\") as f:\n",
    "    images = glob(\"face_labeling_job_images/*.png\")\n",
    "    for image in images:\n",
    "        s3_ref = {\"source-ref\":os.path.join(s3_root_path, image)}\n",
    "        f.write(f\"{json.dumps(s3_ref)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next we upload the image data to S3\n",
    "!aws s3 cp labeled {labeled_data_path} --recursive --quiet\n",
    "!aws s3 cp face_labeling_job_images {job_data_path} --recursive --quiet\n",
    "!aws s3 cp gt_config {labeling_job_config_path} --recursive --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a Ground Truth Labeling Job\n",
    "#### We'll use a private team to avoid any charges and get a better feel for the labeling user experience. First we need to setup a private team. Please follow the steps below:\n",
    "1. Find the SageMaker Service in the AWS Management Console \n",
    "<img src=\"notebook_images/LJ1.JPG\">\n",
    "\n",
    "2. In the SageMaker console under Ground Truth, click **Labeling workforces**\n",
    "<img src=\"notebook_images/LT1.JPG\">\n",
    "\n",
    "3. Click **Private**\n",
    "4. Click **Create private team**\n",
    "5. Name your team **test-team** and provide your email address for both worker and contact. Fill out the rest as per below\n",
    "<img src=\"notebook_images/LT4.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteams = sm_client.list_workteams()[\"Workteams\"]\n",
    "team_arn = [wt[\"WorkteamArn\"] for wt in workteams if wt[\"WorkteamName\"] == \"test-team\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm_client.create_labeling_job(\n",
    "    LabelingJobName=f\"{labeling_job_name}\",\n",
    "    LabelAttributeName=f\"{labeling_job_name}\",\n",
    "    InputConfig={\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"ManifestS3Uri\": f\"{labeling_job_config_path}/input.manifest\"\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    OutputConfig={\n",
    "        \"S3OutputPath\": f\"{labeled_data_path}\",\n",
    "    },\n",
    "    RoleArn=f\"{role}\",\n",
    "    LabelCategoryConfigS3Uri=f\"{labeling_job_config_path}/label_config.json\",\n",
    "    StoppingConditions={\n",
    "        \"MaxHumanLabeledObjectCount\": 200,\n",
    "        \"MaxPercentageOfInputDatasetLabeled\": 100\n",
    "    },\n",
    "\n",
    "    HumanTaskConfig={\n",
    "        \"WorkteamArn\": f\"{team_arn}\",\n",
    "        \"UiConfig\": {\n",
    "            \"UiTemplateS3Uri\": f\"{labeling_job_config_path}/template.liquid\"\n",
    "        },\n",
    "        \"PreHumanTaskLambdaArn\": f\"{pre_annotation_lambdas[region]}\",\n",
    "        \"TaskTitle\": \"ground truth lab\",\n",
    "        \"TaskDescription\": \"facial recognition\",\n",
    "        \"NumberOfHumanWorkersPerDataObject\": 1,\n",
    "        \"TaskTimeLimitInSeconds\": 240,\n",
    "        \"TaskAvailabilityLifetimeInSeconds\": 240,\n",
    "        \"MaxConcurrentTaskCount\": 200,\n",
    "        \"AnnotationConsolidationConfig\": {\n",
    "            \"AnnotationConsolidationLambdaArn\": f\"{consolidation_lambdas[region]}\"\n",
    "        },\n",
    "\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Now that we have a nice labeled dataset to work with. We can begin training our model\n",
    "\n",
    "We'll train a model emloying a Siamese network architecture where we pass in two images as inputs and the model attempts to minimize the distance based loss function to bring similar images together and push dissimilar images appart\n",
    "\n",
    "<img src=\"notebook_images/siamese_network.jpg\" width=600>\n",
    "\n",
    "[Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll build our model with Tensorflow. SageMaker has a managed container that can be used to run managed training jobs.\n",
    "# To run a training job with Sagemaker, you just need to provide a script that trains the model and saves a the model artifact to a specified directory\n",
    "# Let's take a look at the script\n",
    "!pygmentize -l python src/training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the training job\n",
    "model = TensorFlow(entry_point='src/training.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='local_gpu',\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py3',\n",
    "                             hyperparameters = {\"epochs\": 20, \"steps_per_epoch\":32}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on the labeled data - this will copy all of the data in the labeled data path onto the training instance\n",
    "model.fit(inputs=labeled_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will deploy the model as a REST endpoint using TF Serving\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_image(path):\n",
    "    \"prep image for inference\"\n",
    "    im = Image.open(path)\n",
    "    im = np.array(im)\n",
    "    im = im[:, (im != 255).sum(axis=0) > 50]\n",
    "    im = np.array(Image.fromarray(im).resize((92,112)))\n",
    "    im = im / 255\n",
    "    return im[None,:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_output(f1_sub, f1_expr, f2_sub, f2_expr, thresh=0.5):\n",
    "    \n",
    "    \"Show images along with the dissimilarity score\"\n",
    "    \n",
    "    path1 = f\"test_images/{f1_sub}.{f1_expr}\"\n",
    "    path2 = f\"test_images/{f2_sub}.{f2_expr}\"\n",
    "    \n",
    "    im1 = prep_image(path1)\n",
    "    im2 = prep_image(path2)\n",
    "    \n",
    "    # prepare input for TF Serving Endpoint\n",
    "    inputs = {\n",
    "      'instances': [\n",
    "        {\"input_top\":im1.tolist(),\n",
    "        \"input_bottom\":im2.tolist()},\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    prediction = predictor.predict(inputs)['predictions'][0][0]\n",
    "    \n",
    "    img1 = Image.open(path1)\n",
    "    img2 = Image.open(path2)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,7))\n",
    "    axes[0].imshow(img1, cmap='gray')\n",
    "    axes[1].imshow(img2, cmap='gray')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    same_dif = \"same\" if prediction < thresh else \"different\"\n",
    "    \n",
    "    fig.suptitle(f\"Dissimilarity Score = {prediction:.3f}\\n Likely {same_dif} person\", y=0.85, size=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions = ['surprised', 'sleepy', 'glasses', 'normal', \n",
    "               'sad', 'wink', 'centerlight', 'happy', 'noglasses']\n",
    "\n",
    "subjects = [f\"subject{x:02d}\" for x in range(1,16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output(f1_sub = random.choice(subjects), f1_expr = random.choice(expressions),\n",
    "           f2_sub = random.choice(subjects), f2_expr = random.choice(expressions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_output(f1_sub = subjects[14], f1_expr = expressions[2],\n",
    "           f2_sub = subjects[8], f2_expr = expressions[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the endpoint \n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
